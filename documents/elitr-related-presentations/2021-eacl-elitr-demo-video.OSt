Hello!
I'm Ond≈ôej Bojar and I would like to present today the demo of the European project ELITR, or European Live Translator.
ELITR is a research and innovation action that has multiple goals.
But I'm going to focus only on the first one, which is highly multilingual machine and speech translation.
The project is coordinated by Charles University in Prague, with two research partners more.
The University of Edinburgh and Karlsruhe Institute of Technology.
We have one integration partner PerVoice, and one user partner within the project, which is alphaview, the teleconferencing company.
We also have one external, or affiliated user partner.
And that's the Supreme Audit Office of the Czech Republic.
They were actually those who initiated the idea of the project.
They were supposed to run a congress in May 2020 for EUROSAI, the association of European supreme audit institutions, and they wanted to provide that congress with subtitles in many more languages.
From six source languages in speech, we are supposed to produce translations in 43 target languages, which includes 24 EU languages, and 19 more EUROSAI languages, such as Armenian, Russian, Bosnian, Georgian, Hebrew, Kazakh, Norwegian or Luxembourgish.
In the project we're also investigating the possibilities of going directly from the source speech into the target language with an end-to-end spoken language translation system.
But the production pipeline today still consists of the two independent steps: ASR (speech recognition), and machine translation.
We are actually quite good in these two steps.
So, the Charles University has recently reached the human quality in translation of news from English into Czech, and even more recently, the Karlsruhe Institute of Technology has reached superhuman performance in precision of recognizing speech in the online mode with a very low latency.
So, what happens when you put these two near human or superhuman technologies together? In the first demo, the sound is actually coming from the video and the video at the same time is also sent to the users who watch our subtitles.
Note that the path for the video and the path for the automatic recognition and translation and then the presentation of the subtitles is different.
And from that difference we actually gain a lot of time so the video streaming itself is slower than our pipeline.
[Music]
So, that was the first demo of our technology, where the output was presented in the form of subtitles with very limited space.
And that limited space is acceptable for speech recognition, but it's much more difficult to squeeze in the translation as it's coming in time.
So in the second demo, we're going to present the other presentation type, which we call the paragraph view, and this demo comes from an event which actually took place in reality, and there was even an interpreter present.
So, there was some discussion held in Czech that was interpreted by a human interpreter into English.
And our system got this input from the English interpreter and provided the translations into many other languages including back to Czech as well.
This event was also streamed through the Internet, and again, the delay of the video streaming was actually larger than the delay caused by the combination of the interpreter and the systems that we are preparing.
Although it is dada, you know, it doesn't... maybe not... so coherent... But it makes sense.
And I really think that that... that little plot, that little story is very interesting and very nice.
It has a beginning.
It has an end.
And with all these situations that we had generated by the computer, it actually gives you a micro story of the human.
And the robot is just either an observer, or a catalyst.
And we see the world and through the people.
I hope you liked our demos.
If you would like to see some more, please check out our blog on our webpage elitr.eu.
And I'm going to briefly talk about the strategy for deployment and integration.
So, obviously we are doing standard improvements of the individual components, the speech recognition, and the machine translation, and the multilingual machine translation system.
But what is critical for the success is the endless loop of testing in real situations, and also the evaluation of the quality and of the latency of that system on a diverse test set.
So, for the test events, the original project proposal actually planned only three such events with the Supreme Audit Office of the Czech Republic.
And these are the bold-faced.
But we realized very soon that we need many more practical tests including all the hardware issues that we can run into and so on.
And indeed, at each of these test events we have learned something very important and quite new.
So, there were bugs and errors of all types.
And we, one by one, managed to resolve them somehow.
And for the evaluation of the systems we realize that the existing data sets are too narrow in domains and the quality of speech, and they do not reflect the conditions that we have to face at our events.
So, we started collecting something which we call ELITR test-set.
Please check it out at GitHub.
And there we focus on the continuous growth, so we keep adding material and we keep refining it.
And still, we want to ensure full reproducibility of the results, so we rely on git commit IDs.
And when you are reporting any numbers from this ELITR test-set, make sure to include the commit ID so that everybody knows what exact documents were there.
Please also see our other demo at EACL 2021.
SLTev, the evaluation toolkit which makes the use of this test set rather easy.
So, that was it.
I hope you liked our demos.
Where possible, we release the components and data for public use and we are definitely happy for any type of collaboration.
So please get in touch.
See our contacts at the website elitr.eu.
