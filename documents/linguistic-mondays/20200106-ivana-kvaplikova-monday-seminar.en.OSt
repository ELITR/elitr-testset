Good afternoon.
Thank you for attending my talk. 
Ok, one technical thing, how can I move the slides?
Just from here or is there a ...
Ok, I will just do it from here.
Ok so, in this talk, in the beginning, I will give you a little bit of background about unsupervised machine translation.
I will talk about the importance of cross-lingual pre-training.
Then I will go on to uh, to describe the training pipeline of unsupervised MT models.
And finally, I will share with you some of the results of experiments that I have done in the recent months, and I will present you the the research plan that I have for eh for my Phd.
So um, the topic of my dissertation is "Towards machine translation based on monolingual corpora" which in other words, means unsupervised machine translation.
Um my supervisor is Ondra Bojar and I'm currently in the second year of my PhD.
So, um, to give you the idea behind unsupervised machine translation.
So traditionally, we train machine translation systems on examples of translated sentences, which were already generated by, by humans ideally professional translators.
Uh, the problem is that for many languages around the world, or for many language pairs, these pre-translated expenses, eh, these pre-translated sentences are not to not available, and they would be very expensive to to get.
Because, eh, to have eh eh parallel corpus to be able to train machine translation system, we need to have a substantial amount of texts translated by professional translators.
Therefore there is a lot of motivation to design a system that would train itself only from monolingual texts, which are usually easily available for uh for most languages.
Um, this this topic is quite a new one.
So around two years ago, it still seemed like a crazy idea that this could ever work.
But now research, recent research has shown that actually it is possible to train a mas machine translation system only from monolingual texts.
Ahm okay.
So how how does it work.
How do we do it that we um move from two unrelated, unrelated Mmnolingual corpora to ah, to, the cross lingual space of machine translation.
Uh, so the first approach to enter the cross-lingual space is to train, uh, monolingual embeddings, in two languages independently.
And later, align these two embeddings space by some, eh, some kind of a mapping.
There are several unsupervised ways how to learn this mapping.
One of them is eh is based on an adversarial learning, eh, adverse adversarial training.
And the other one is based on some, focusing on some anchor anchor words, which are identical eh in the two languages.
And then using iterative self-learning to to improve the mapping.
Uh, the underlying assumption of this approach is that eh is that monolingual embeddimg spaces are isomorphic, which, which means that they uh, that the structure of the embeddings spaces is approximately the same across languages.
It is schematically illustrated in the in the, in the picture here and in practice this eh, this wor, is not, was not proved to work a 100 percent for all language pairs but approximately it should hold.
A here you can see sort of a real life, exp example, where we were aligning a Czech and the German monolingual space using the iterative self-learning approach.
And I 'm showing it here on an example of of the Czech and German spaces and focusing on the word "sagte" which is the German translation of the word "said".
And here by eh by looking into the and the nearest neighbors of this of this word you can see that all the words around are somehow related to the uh, to the activity of both saying something or claiming or announcing something.
And now the fact that this holdsmonolingually is a property of the Word2vec model, which was used to general generate the monolingual embeddings.
But the fact that within these nearest neighbors we can actually see the real translations, the correct translations of the word "said" or "sagte" which in Czech is "Å™ekl", that is the consequence of of a correct mapping 
Ah from a from a monolingual from a cross-lingual space like this, it if possible to derive, ah, oh, like a bilingual lexicon from from these examples.
And these can be directly used to bootstrap an unsupervised phrase-based system.
Oh, another another type of cross-lingual pre-training that I 'm going to be talking about is not pre-training only the the the word embeddings" but actually  pre-training an entire language model. 
So the primary examples of cross-lingual language model are XLM and M-BERT.
They are both, they are both cross-lingual language models, which means that they were trained on sentences in different languages, but uh, these, uh, these sentences are not aligned in any way, it is just it is just unrelated sentences concatenated together.
And then using the using this training data to train a language model, the authors are able to to train oh, oh, a cross-lingual language model, which understands the structure of of of all the languages that it has been trained on.
The training objective of this model is the masked language model objective, which means that the model is trained on sentences where where we first mask  some random tokens from from the input sentences, and we train the model to um to be able to to predict the the the missing words and fill in the blanks.
In that way, the the model learns how to predict each word not only from their left context, but from actually the bilingual cont eh the bi the bidirectional context, which which seems to be an important factor for the cros-lingual ability of the model.
Ehm what is what is important to note here is that the model is not trained with any kind of a cross-lingual objective.
So, um, there is no explicit alignment.
And the model is free to choose how to represent the languages, um, which could be in like a differ separate subspace, or or somehow jointly.
And uh, this this question of how the model actually represents the individual sentences and whether there is some cross lingual knowledge that that.
That is an attractive research question that that that was asked whether this model, the multilingual BERT is actually multilingual.
The answer seems to be uh, seems to be yes.
It was shown on, it was showed on on some downstream downstream tasks, such as named entity recognition or or natural language inference that ehm that the model is actually able to to transfer the knowledge between sentences between languages.
And when we take the pretrained cross-lingual model and we fine-tune it, for example, on English data, to to recognize named entities.
Then later at inference time, the model is able to ehm to transfer this knowledge that it learned on English names, to transfer it to different languages and recognize, for example, German names or names in other languages.
So this seems to be a proof of some cross-lingual transfer of some cross-lingual knowledge of the model.
Another way to look at it is to actually somehow visualize the uh, the internal representations of of the model.
For example, here in the picture you, you see something like, oh, oh, positive example of of um language agnostic representations, where the uh where the nearest neighbors of of each sentence in an in a representation space are its translations.
So the representation depends more on on the the meaning of the underlying sentence, rather than uh, the the language it was originally expressed in.
Um.
We wanted to explore this, and this is the work that I was doing uh, during my research stay in Spain.
We were exploring how often is this the case that the model ehm represents the ah the words or sentences in some sort of a language neutral way.
So ah, we took um 3000 parallel sentences, 3000 sentences translated in English, German and Czech.
And we um, we wanted to have a look, whether when we encode these mo these sentences, whether the model will be able to match the match the corresponding translation pairs by performing a nearest neighbor search.
Uh.
The results of this task, so task of matching parallel sentences, is here in this chart.
So um.
It shows the like the success rate of the model to to recover these translation pairs, ah across across its layers.
So we were looking at the representations, the internal representations that uh the multilingual model has for, for the encoded sentences.
And we, we saw that when we actually look at how does M-BERT represent the sentences at its deeper layers, around fifth-to-last layer.
And we use these representations to to try to find the translation pairs, that in 90 percent of cases we are able to.
Um, this ability is, oh, of course, also depends a little bit on on different languages.
It seems to be that for some languages it is easier to to cross-lingually align the representations, for some language pairs it seems to be more difficult .
Like for Czech and English for example, it shows that the eh the eh the alignment is not as successful.
That leads to another question eh about like where does this cross-lingual ability actually come from given that the models are not trained with any cross-lingual objective.
And what does it depend on.
So the initial hypothesis was that an important factor is the number of overlapping words in, uh, in the two sentences that we are trying to, ehm in the two languages that we are trying to to model.
Uh, that means, for example, for example, names or numbers or some URLs.
And this hypothesis was actually later disproved, because eh the authors tried to actually train a cross-lingual model on two languages with exactly zero word overlap and it turns out that the cross-lingual transfer is is still there.
So the word overlap is not eh the likely cause of the eh cross-lingual ability, the eh more likely ehm source of this ability is in the structural similarity of of the languages, so their morphology, word ordering or word frequen frequencies.
Also an important eh an important factor of eh of this eh cross-lingual pre-training is that the eh that there is a parameter sharing between for when modeling the two two languages.
So that is, that seems to be the uh, the, one of the most important factors together with the depth of the of the network.
OK, so, this was about how do we even enter the cross-lingual space from the monolingual coropra.
And now I would like to describe how we actually train the translation model itself.
So eh in case of unsupervised neural machine translation models, um, we have an encoder decoder eh architecture, where we use the pretrained XLM model to initialize both the encoder and the and the decoder.
The model is multilingual so the encoder and decoder they are used for both for both translation direction.
So.
For example, when translating from German to Czech and Czech to German, we are using the the same encoder
Ahh so once we initialize it, we fine-tune it on on two tasks.
One of them is back-translation and the other one is denoising.
Ah back-translation means that we use the model to generate its own training data.
Um, uh, which means that we first use it to translate to translate a batch of sentences.
And then we use these this batch of of like synthetic translation pairs to train the model in the other translation direction.
So in this way we are able to iteratively improve the quality during training.
Uh, the other subtask is called denoising.
Uh, it's just a multilingual objective whereby we are training the model to eh reconstruct a corrupted sentence.
So we feed the model with a some sort of noised versions of of input data, and we train it to to reconstruct the the correct the correct sentences.
Am this objective teaches the model to understand the structure of the of the language and to to produce more fluent outputs.
Ah, in ah, an unsupervised phrase-based model, that's another, that's another very different approach.
We um, we use what I already mentioned in the mentioned earlier when talking about the cross-lingual embeddings.
So we use those to to generate a lexicon or a phrase table.
Ah, we can also use the cosine distances between different words as an indicator of the probability of these of these translation pairs.
And that is already the most important, the most important component of the oh, the unsupervised phrase-based model.
The other important component is the language model, which we trained on the target language.
But there uh, there is no difference from the supervised scenario, because language model is all always trained on a monolingual data, um, so there is there is nothing special there.
And ah just from these two two components and some other features of of phrase-based modeling we are able to uh, get an initial phrase-based model which can later be also improved, similarly to the neural model, with back-translation, so using the reverse model to to generate sentences that this model is later trained on.
Em ok.
So now I would like to show you eh how do these models that I have just now described, how do they perform in practice.
We measured this on the Czech-German language pair.
Ah, we are comparing here the unsupervised phrase-based model that I was now talking about with the unsupervised neural model eh initialized from XLM.
And finally, eh one more model, which is a neural one, and it is it is trained on a synthetic parallel corpus, which was generated by the the first first model by the phrase-based one.
So we used the phrase-based model to generate a a synthetic corpus and then we use it to train a supervised NMT model.
Um.
So from these three, ah, the highest blue blue scores are achieved by by the purely neural one, eh which, eh which is, oh, the blue score that we achieved from German to Czech is 16.
But then when we compare ourselves to the winner of the WMT unsupervised task from last year, there is still quite some discrepancy there.
They achieved 17 point 8 and what they did is that they actually combined, the, both the pre-training by XLM, and the ah ah training on the on the synthetic corpus from unsupervised phrase-based model.
So it is kind of like a combination of combination of of the two showing that these hybrid models actually have quite a lot of potential for further improving for further improving translation quality.
Also, what is interesting to see here is that this this um model from LMU actually performs quite a bit better than when we translate the, when we translate the sentences via English.
So when we use two models and translate in sequence.
Also, what can be observed from here is that the when training a supervised benchmark, it achieved around 21, ah, 20 point 8 blue score for German to Czech, and which means that the oh the difference, or the gap between unsupervised and supervised MT is really getting, uh, is really getting smaller.
Ah, yeah, here to to to not get overly optimistic about the results.
I I prepared a couple of sent or one sentence, translated by by the models that that I trained.
And what you can see here is that the different models make different mistakes.
So for example, the phrase-based models seem to have a big issue with with names with named entites.
So here the, all of these are translations of of a sentence from a newspaper article about the hurricane Rosa which is weakening over the cooler waters of the north coast of meh Mexico.
And here in um, in the phrase-based models, it is they are translating the the name completely wrong as Milada or something else.
And however, however, then when we look at neural based model, the neural model, that one is getting the the name right but it is actually, um, the the rest of the sentence is probably even worse than the others.
It seems to be be um hallucinating some some extra words into the sentence.
So from from from this it seems that yeah, the uh, the models are making different mistakes.
So maybe uh, combining them combining them all could have oh, and the some other research work already showed it, that it could have some positive impact on the final translation quality.
So to conclude, I would like to tell you what I ehm what I ehm what my plan is what I would now like to focus on.
So yeah, as I was saying I would like to focus on the oh, on some kind of effective combination of phrase-based and neural unsupervised models.
Then I would like to look more into the into the proper alignment of language representation spaces because the examples that I that I showed, they they are showing that um that the alignment is possible, and it is somehow working, but there is definitely room for a room for improvement.
And finally, I would like to ah, oh, conclude what I what I already started, and maybe look deeper in it.
That is the cross-lingual sentence embeddings.
So um, I would like to see whether it is possible to use these for maybe filtering comparable comparable corpora, or may or maybe for, um, oh, maybe to use them during unsupervised MT training, for example, to score the synthetic translations ehm generated during MT training, uh, for yeah, for the unsupervised systems.
Ok, so that's it, thank you for attention.
